\chapter{Overview Of Machine Learning Algorithms} \label{sec:ML_BGK}
Machine learning is evolved from a collection of powerful techniques in AI areas. This new methods allows the system to learn useful structural patterns and models from training data.
A machine learning approach consists of two main phases: training phase and decision making phase. At the training phase, after a data mining period of system input/output information, machine learning methods are applied to learn the system model using the training dataset. At the decision making phase, the system can obtain the estimated output for each new input by using the trained model.
Machine learning algorithms can be distinguished into four main categories: supervised, unsupervised, semi-supervised and reinforcement learning.
\begin{figure}[tb!]
	\centering
	\includegraphics[width=13cm]{figure/ML_algo.png}
	\caption{Common machine learning algorithms.}
	\label{fig:{ML_algo}}
\end{figure}
Each algorithm in Figure \ref{fig:{ML_algo}} is briefly explained with some examples. For a more insightful discussion on machine learning theory and its classical concepts, please refer to \cite{Mohammed2016, Marsland2015, Alpaydin2020}.
\begin{itemize}
\item[]\textbf{A. Supervised Learning}\\
Supervised learning is a kind of labelling learning technique. Supervised learning algorithms are given a labeled training dataset (i.e., inputs and known outputs) to build the system model representing the learned relation between the input and output. After training, when a new input is fed into the system, the trained model can be used to get the expected output \cite{Kotsiantis2007, Hastie2009}. In the following, we will give a detailed representation of supervised learning algorithms.
\begin{itemize}
\item[]\textit{1) k-Nearest Neighbor} (k-NN): In k-NN the classification of a data sample is determined based on the k nearest neighbors of that unclassified sample. The process of the k-NN algorithm is very simple: if the most of the k nearest neighbors belong to a certain class, the unclassified sample will be classified into that class. The higher the value of k is, the less effect the noise will have on the classification. Since the distance is the main metric of the k-NN algorithm, several functions can be applied to define the distance between the unlabeled sample and its neighbors, such as Chebyshev, City-block, Euclidean and Euclidean squared \cite{Cover1967}.
\item[]\textit{2) Decision Tree} (DT): The DT performs classification through a learning tree. In the tree, each node represents a data feature, all branches represent the conjunctions of features that lead to classifications, and each leaf node is a class label. The unlabeled sample can be classified by comparing its feature values with the nodes of the decision tree \cite{Han2011}. The DT has many advantages, such as intuitive knowledge expression, simple implementation and high classification accuracy. ID3 \cite{Quinlan1986}, C4.5 \cite{Karatsiolis2012} and CART \cite{Burrows1995} are three widely-used decision tree algorithms. The biggest difference among them is the splitting criteria which are used to build decision trees. 
\item[]\textit{3) Random Forest} (RF): A RF \cite{Breiman1999} consists of many DT. To mitigate over-fitting of DT method and improve accuracy, the random forest method randomly chooses only a subset of the feature space to construct each DT. The steps to classify a new data sample by using random forest method are:
\begin{itemize}
\item[]a) put the data sample to each tree in the forest;
\item[](b) Each tree gives a classification result, which is the tree’s “vote”;
\item[](c) The data sample will be classified into the class which has the most votes.
\end{itemize}
\item[]\textit{4) Neural Network} (NN): A neural network is a computing system composed by a large number of simple processing units, which operate in parallel to learn experiential knowledge from historical data \cite{Haykin}. Each neuron perform highly complex, nonlinear and parallel computations. In a NN, its nodes are the equivalent components of the neurons in the human brain. These nodes use activation functions to perform nonlinear computations. The most frequently used activation functions are the sigmoid and the hyperbolic tangent functions. Simulating the way neurons are connected in the human brain, the nodes in a NN are connected to each other by variable link weights.
A NN has many layers. The first layer is the input layer and the last layer is the output layer and layers between them are the hidden layers. The output of each layer is the input of the next layer and the output of the last layer is the result. By changing the number of hidden layers and the number of nodes in each layer, complex models can be trained to improve the performance of NNs. NNs are widely used in many applications, such as pattern recognition. In figure \ref{fig:{NN_base}} the most basic NN with three layers has been shown. An input has $m$ features (i.e., $X_{1},X_{2},...,X_{m}$) and the input can be assigned to n possible classes (i.e., $Y_{1},Y_{2},...,Y_{n}$). Also, $W_{ij}^{1}$ denotes the variable link weight between the $ith$ neuron of layer $l$ and the $jth$ neuron of layer $l + 1$, and $ak^{l}$ denotes the activation function of the $kth$ neuron in layer $l$.
\begin{figure}[tb!]
	\centering
	\includegraphics[width=13cm]{figure/NN_base.png}
	\caption{A basic neural network with three layers: an input layer, a hidden layer and an output layer.}
	\label{fig:{NN_base}}
\end{figure}
There are many types of neural networks, which can be divided in supervised or unsupervised main group \cite{Lee2005}. In the following, we will give a brief description of supervised neural networks.
\begin{itemize}
\item[]\textit{a)	Random NN}: The random NN can be represented as an interconnected network of neurons which exchange spiking signals. The main difference between random NN and other neural networks is that neurons in random NN exchange spiking signals probabilistically. In random NN, the internal excitatory state of each neuron is represented by an integer called “potential”. The potential value of each neuron rises when it receives an excitatory spiking signal and drops when it receives an inhibitory spiking signal. Neurons whose potential values are strictly positive are allowed to send out excitatory or inhibitory spiking signals to other neurons according to specific neurondependent spiking rates. When a neuron sends out a spiking signal, its potential value drops one. The random NN has been used in classification and pattern recognition \cite{Timotheou2010}.
\item[]\textit{b)	Deep NN}: Neural networks with a single hidden layer are generally referred to as shallow NNs. In contrast, neural networks with multiple hidden layers between the input layer and the output layer are called deep NNs \cite{LeCun2015, Schmidhuber2015}. To process high-dimensional data and to learn increasingly complex models, deep NNs with more hidden layers and neurons are needed. However, deep NNs increase the training difficulties and require more computing resources. In recent years, the development of hardware data processing capabilities and the evolved activation functions make it possible to train deep NNs \cite{Pandey2014}. In deep NNs, each layer’s neurons train a feature representation based on the previous layer’s output, which is known as feature hierarchy. The feature hierarchy makes deep NNs capable of handling large high-dimensional datasets. Due to the multiple-level feature representation learning, compared to other machine learning techniques, deep NNs generally provide much better performance \cite{Pandey2014}.
\item[]\textit{c)	Convolutional NN}: Convolutional NN and recurrent NN are two major types of deep NNs. Convolutional NN \cite{Krizhevsky2012, Li2018} is a feed-forward neural network. Local sparse connections among successive layers, weight sharing and pooling are three basic ideas of convolutional NN. Weight sharing means that weight parameters of all neurons in the same convolution kernel are same. Local sparse connections and weight sharing can reduce the number of training parameters. Pooling can be used to reduce the feature size while maintaining the invariance of features. The three basic ideas reduce the training difficulties of convolutional NNs greatly.
\item[]\textit{d)	Recurrent NN}: In feed-forward neural networks, the information is transmitted directionally from the input layer to the output layer. However, recurrent NN is a stateful network, which can use internal state (memory) to handle sequential data. Unlike a traditional deep NN, which uses different parameters at each layer, the recurrent NN shares the same parameters across all time steps. This means that at each time step, the recurrent NN performs the same task, just with different inputs. In this way, the total number of parameters needed to be trained is reduced greatly. Long Short-Term Memory (LSTM) \cite{Li2015a} is the most commonly-used type of recurrent NNs, which has a good ability to capture long-term dependencies. LSTM uses three gates (i.e., an input gate, an output gate and a forget gate) to compute the hidden state.
\end{itemize}
\item[]\textit{5) Support Vector Machine} (SVM): SVM is invented by Vapnik and others \cite{Vapnik1999}, which has been widely used in classification and pattern recognition. The basic idea of SVM is to map the input vectors into a high-dimensional feature space. This mapping is achieved by applying different kernel functions, such as linear, polynomial and Radial Based Function (RBF). Kernel function selection is an important task in SVM, which has effect on the classification accuracy. The selection of kernel function depends on the training dataset. The linear kernel function works well if the dataset is linearly separable. If the dataset is not linearly separable, polynomial and RBF are two commonly-used kernel functions. In general, the RBF-based SVM classifier has a relatively better performance than the other two kernel functions.
The objective of SVM is to find a separating hyperplane in the feature space to maximize the margin between different classes. The margin is the distance between the hyperplane and the closest data points of each class. The corresponding closest data points are defined as support vectors.
\item[]\textit{6) Bayes’ Theory}: Bayes’ theory uses the conditional probability to calculate the probability of an event occurring given the prior knowledge of conditions that might be related to the event. The Bayes’ theory is defined mathematically as the following equation:
\begin{equation*}
P(H\vert E)=\dfrac{P(E\vert H)P(H)}{P(E)}
\end{equation*}
where $E$ is a new evidence, $H$ is a hypothesis, $P(H\vert E)$ is the posterior probability that the hypothesis $H$ holds given the new evidence $E$, $P(E\vert H)$ is the posterior probability that of evidence $E$ conditioned on the hypothesis $H$, $P(H)$ is the prior probability of hypothesis $H$, independent of evidence $E$, and $P(E)$ is the probability of evidence $E$.
In a classification problem, the Bayes’ theory learns a probability model by using the training dataset. The evidence $E$ is a data sample, and the hypothesis $H$ is the class to assign for the data sample. The posterior probability $P(H\vert E)$ represents the probability of a data sample belonging to a class. In order to calculate the posterior probability $P(H\vert E)$, $P(H)$, $P(E)$ and $P(E\vert H)$ need to be calculated first based on the training dataset using the probability and statistics theories, which is the learning process of the probability model. When classifying a new input data sample, the probability model can be used to calculate multiple posterior probabilities for different classes. The data sample will be classified into the class with the highest posterior probability $P(H\vert E)$. The advantage of the Bayes’ theory is that it requires a relatively small number of training dataset to learn the probability model \cite{Box2011}. However, there is an important independence assumption when using the Bayes’ theory. To facilitate the calculation of $P(E\vert H)$, the features of data samples in the training dataset are assumed to be independent of each other \cite{Bakker2017}.
\item[]\textit{7) Hidden Markov Models} (HMM): HMM is one kind of Markov models. Markov models are widely used in randomly dynamic environments which obey the 
memoryless property. The memoryless property of Markov models means that the conditional probability distribution of future states only relates to the value of the current state and is independent of all previous states \cite{Rabiner1989, Holgado2020}. There are other Markov models, such as Markov Chains (MC). The main difference between HMM and other models is that HMM is often applied in environments where system states are partially visible or not visible at all.
\end{itemize}
\item[]\textbf{B. Unsupervised Learning}
In contrast to supervised learning, an unsupervised learning algorithm is given a set of inputs without labels, thus there is no output. Basically, an unsupervised learning algorithm aims to find patterns, structures, or knowledge in unlabeled data by clustering sample data into different groups according to the similarity between them. The unsupervised learning techniques are widely used in clustering and data aggregation. In the following, we will give a representation of widely-used unsupervised learning algorithms.
\begin{itemize}
\item[]\textit{1)	k-Means}: The k-means algorithm is used to recognize a set of unlabeled data into different clusters. To implement the kmeans algorithm, only two parameters are needed: the initial dataset and the desired number of clusters. If the desired number of clusters is k, the steps to resolve node clustering problem by using k-means algorithm are:
\begin{itemize}
\item[]\textit{a)} initialize k cluster centroids by randomly choosing k nodes;
\item[]\textit{b)} use a distance function to label each node with the closest centroid;
\item[]\textit{c)} assign new centroids according to the current node memberships;
\item[]\textit{d)} stop the algorithm if the convergence condition is valid, otherwise go back to step \textit{b)}.
\end{itemize}
\item[]\textit{2)	Self-Organizing Map} (SOM): SOM, also known as SelfOrganizing Feature Map (SOFM) \cite{Kohonen2012}, is one of the most popular unsupervised neural network models. SOM is often applied to perform dimensionality reduction and data clustering. In general, SOM has two layers, an input layer and a map layer. When SOM is used to perform data clustering, the number of neurons in the map layer is equal to the desired number of clusters. Each neuron has a weight vector. The steps to resolve data clustering problem by using SOM algorithm are:
\begin{itemize}
\item[]a) initialize the weight vector of each neuron in the map layer;
\item[](b) choose a data sample from the training dataset;
\item[](c) use a distance function to calculate the similarity between the input data sample and all weight vectors. The neuron whose weight vector has the highest similarity is called the Best Matching Unit (BMU). The SOM algorithm is based on competitive learning;
\item[](d) The neighborhood of the BMU is calculated;
\item[](e) The weight vectors of the neurons in the BMU’s neighborhood are adjusted towards the input data sample;
\item[](f) Stop the algorithm if the convergence condition is valid, otherwise go back to step (b).
\end{itemize}
\end{itemize}
\item[] \textbf{C. Semi-Supervised Learning}
Semi-supervised learning is a type of learning which uses both labeled and unlabeled data. Semi-supervised learning is useful due the fact that in many real-world applications, the acquisition of labeled data is expensive and difficult while acquiring a large amount of unlabeled data is relatively easy and cheap. Moreover effective use of unlabeled data during the training process actually tends to improve the performance of the trained model. In order to make the best use of unlabeled data, assumptions have to be hold in semisupervised learning, such as smoothness assumption, cluster assumption, low-density separation assumption, and manifold assumption. Pseudo Labeling \cite{Wu2018} is a simple and efficient semi-supervised learning technique. The main idea of Pseudo Labeling is simple. Firstly, use the labeled data to train a model. Then, use the trained model to predict pseudo labels of the unlabeled data. Finally, combine the labeled data and the newly pseudo-labeled data to train the model again. There are other semi-supervised learning methods, such as Expectation Maximization (EM), co-training, transductive SVM and graph-based methods. Different methods rely on different assumptions. For example, EM builds on cluster assumption, transductive SVM builds on low-density separation assumption, while graph-based methods build on the manifold assumption.
\item[]\textbf{D. Reinforcement Learning}
\begin{itemize}
\item[]\textit{1)	Reinforcement Learning} (RL): RL \cite{Sutton2018, Kaelbling1996} involves an agent, a state space S and an action space A. The agent is a learning entity which interacts with its environment to learn the best action to maximize its long-term reward. The long-term reward is a cumulative discounted reward and relates to both the immediate reward and future rewards. When applying RL to SDN, the controller generally works as an agent and the network is the environment. The controller monitors the network status and learns to make decisions to control data forwarding. Specifically, at each time step $t$, the agent monitors a state $s_{t}$ and chooses an action $a_{t}$ from the action space $A$, receives an immediate reward $r_{t}$ which indicates how good or bad the action is, and transitions to the next state $st+1$. The objective of the agent is to learn the optimal behavior policy $\pi$ which is a direct map from the state space S to the action space $A (\pi : S \longrightarrow A)$ to maximize the expected long-term reward. From the behavior policy $\pi$, the agent can determine the best corresponding action given a particular state. In RL, value function is used to calculate the long-term reward of an action given a state. The most well-known value function is Q-function, which is used by Q-learning to learn a table storing all state-action pairs and their long-term rewards.
\item[]\textit{2)	Deep Reinforcement Learning} (DRL): The main advantage of RL is that it works well without prior knowledge of an exact mathematical model of the environment. However, the traditional RL approach has some shortcomings, such as low convergence rate to the optimal behavior policy $\pi$ and its inability to solve problems with high-dimensional state space and action space. These shortcomings can be addressed by DRL. The key idea of DRL is to approximate the value function by leveraging the powerful function approximation property of deep NNs. After training the deep NNs, given a state-action pair as input, DRL is able to estimate the long-term reward. The estimation result can guide the agent to choose the best action.
\item[]\textit{3)	RL-Based Game Theory}: Game theory is a mathematical tool that focuses on strategic interactions among rational decision-makers. A game generally involves a set of players, a set of strategies and a set of utility functions. Players are decision-makers. Utility functions are used by players to select optimal strategies. In cooperative games, players cooperate and form multiple coalitions. Players choose strategies that maximize the utility of their coalitions. In non-cooperative games, players compete against each other and choose strategies individually to maximize their own utility. In the network field, it is often assumed that nodes are selfish.
In non-cooperative games, players do not communicate with each other, and at the beginning of each play round, players do not have any information about the strategies selected by the other players. At the end of each play round, all players broadcast their selected strategies, which are the only external information. However, each player’s utility can be affected by the other players’ strategies. In this case, adaptive learning methods should be used to predict the strategies of the other players, based on which each player chooses its optimal strategy. RL is a widely-used adaptive learning method, which can help players select their optimal strategies by learning from historical information such as network status, the other players’ strategies and the corresponding utility. Thus, RL-based game theory is an effective decision-making technique.
\end{itemize}
\end{itemize}
In summary, supervised learning algorithms are generally applied to conduct classification and regression tasks, while unsupervised and reinforcement learning algorithms are applied to conduct clustering and decision-making tasks respectively.

%====================================================================================================
%====================================================================================================